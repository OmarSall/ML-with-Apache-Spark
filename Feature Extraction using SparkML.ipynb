{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "#Ignore any warnings by SparkSession command\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Feature Extraction and Transformation using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf2097",
   "metadata": {},
   "source": [
    "# Tokenizer - is used to break a sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561fa7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tokenizer\n",
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a121955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sample dataframe\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (1, \"Spark is a distributed computing system.\"),\n",
    "    (2, \"It provides interfaces for multiple languages\"),\n",
    "    (3, \"Spark is built on top of Hadoop\")\n",
    "], [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f33e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the dataframe\n",
    "sentenceDataFrame.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea639175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokenizer instance.\n",
    "#mention the column to be tokenized as inputcol\n",
    "#mention the output column name where the tokens are to be stored.\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac694269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "token_df = tokenizer.transform(sentenceDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the tokenized data\n",
    "token_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98767946",
   "metadata": {},
   "source": [
    "# CountVectorizer -  convert text into numerical format. It gives the count of each word in a given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c514fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import CountVectorizer\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2da559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sample dataframe and display it.\n",
    "textdata = [(1, \"I love Spark Spark provides Python API \".split()),\n",
    "            (2, \"I love Python Spark supports Python\".split()),\n",
    "            (3, \"Spark solves the big problem of big data\".split())]\n",
    "\n",
    "textdata = spark.createDataFrame(textdata, [\"id\", \"words\"])\n",
    "\n",
    "textdata.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3904e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "# mention the column to be count vectorized as inputcol\n",
    "# mention the output column name where the count vectors are to be stored.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ba350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the CountVectorizer model on the input data\n",
    "model = cv.fit(textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7bea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the input data to bag-of-words vectors\n",
    "result = model.transform(textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4bb8fd",
   "metadata": {},
   "source": [
    "# TF - IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c1da5",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequency is used to quantify the importance of a word in a document. TF-IDF is computed by multiplying the number of times a word occurs in a document by the inverse document frequency of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a102bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary classes for TF-IDF calculation\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec9624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sample dataframe and display it.\n",
    "sentenceData = spark.createDataFrame([\n",
    "        (1, \"Spark supports python\"),\n",
    "        (2, \"Spark is fast\"),\n",
    "        (3, \"Spark is easy\")\n",
    "    ], [\"id\", \"sentence\"])\n",
    "\n",
    "sentenceData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the \"sentence\" column and store in the column \"words\"\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc541639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HashingTF object\n",
    "# mention the \"words\" column as input\n",
    "# mention the \"rawFeatures\" column as output\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an IDF object\n",
    "# mention the \"rawFeatures\" column as input\n",
    "# mention the \"features\" column as output\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "tfidfData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81291ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the tf-idf data\n",
    "tfidfData.select(\"sentence\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bec6d3",
   "metadata": {},
   "source": [
    "# StopWordsRemover \n",
    "### StopWordsRemover is a transformer that filters out stop words like \"a\",\"an\" and \"the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045dc110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import StopWordsRemover\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c28ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with sample text and display it\n",
    "textData = spark.createDataFrame([\n",
    "    (1, ['Spark', 'is', 'an', 'open-source', 'distributed', 'computing', 'system']),\n",
    "    (2, ['IT', 'has', 'interfaces', 'for', 'multiple', 'languages']),\n",
    "    (3, ['It', 'has', 'a', 'wide', 'range', 'of', 'libraries', 'and', 'APIs'])\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2765fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from \"sentence\" column and store the result in \"filtered_sentence\" column\n",
    "remover = StopWordsRemover(inputCol=\"sentence\", outputCol=\"filtered_sentence\")\n",
    "textData = remover.transform(textData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe\n",
    "textData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652fefe",
   "metadata": {},
   "source": [
    "# StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92294b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef76f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with sample text and display it\n",
    "colors = spark.createDataFrame(\n",
    "    [(0, \"red\"), (1, \"red\"), (2, \"blue\"), (3, \"yellow\" ), (4, \"yellow\"), (5, \"yellow\")],\n",
    "    [\"id\", \"color\"])\n",
    "\n",
    "colors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the strings in the column \"color\" and store their indexes in the column \"colorIndex\"\n",
    "indexer = StringIndexer(inputCol=\"color\", outputCol=\"colorIndex\")\n",
    "indexed = indexer.fit(colors).transform(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25684cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0860a",
   "metadata": {},
   "source": [
    "# StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f72e6",
   "metadata": {},
   "source": [
    "### StandardScaler transforms the data so that it has a mean of 0 and a standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import StandardScaler\n",
    "from pyspark.ml.feature import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9260509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataframe and display it\n",
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(1, Vectors.dense([70, 170, 17])),\n",
    "        (2, Vectors.dense([80, 165, 25])),\n",
    "        (3, Vectors.dense([65, 150, 135]))]\n",
    "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc90d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StandardScaler transformer\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e7f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the transformer to the dataset\n",
    "scalerModel = scaler.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2976651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaledData = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa718e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the scaled data\n",
    "scaledData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b761a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
