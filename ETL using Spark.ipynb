{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b02de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9549a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ed1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "#Ignore any warnings by SparkSession command\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETL using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e62c1",
   "metadata": {},
   "source": [
    "## Create a Dataframe from the raw data and write to CSV file.Â¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db78aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of tuples\n",
    "#each tuple contains the student id, height and weight\n",
    "data = [(\"student1\",64,90),\n",
    "        (\"student2\",59,100),\n",
    "        (\"student3\",69,95),\n",
    "        (\"\",70,110),\n",
    "        (\"student5\",60,80),\n",
    "        (\"student3\",69,95),\n",
    "        (\"student6\",62,85),\n",
    "        (\"student7\",65,80),\n",
    "        (\"student7\",65,80)]\n",
    "\n",
    "# some rows are intentionally duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe using createDataFrame and pass the data and the column names.\n",
    "\n",
    "df = spark.createDataFrame(data, [\"student\",\"height_inches\",\"weight_pounds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa383ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the data frame\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69872b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"student-hw.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student-hw.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student-hw.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff20527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows in the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191bbddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9713ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2750f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data to a Parquet file\n",
    "df.write.mode(\"overwrite\").parquet(\"student-hw.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecf1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the number of partitions in the dataframe to one.\n",
    "df = df.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63147766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the data to a Parquet file\n",
    "df.write.mode(\"overwrite\").parquet(\"student-hw-single.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b741502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"student-hw-single.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a0ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the expr function that helps in transforming the data\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b098377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inches to centimeters\n",
    "# Multiply the column height_inches with 2.54 to get a new column height_centimeters\n",
    "df = df.withColumn(\"height_centimeters\", expr(\"height_inches * 2.54\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51689e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pounds to kilograms\n",
    "# Multiply weight_pounds with 0.453592 to get a new column weight_kg\n",
    "df = df.withColumn(\"weight_kg\", expr(\"weight_pounds * 0.453592\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a5bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns \"height_inches\",\"weight_pounds\"\n",
    "df = df.drop(\"height_inches\",\"weight_pounds\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d610a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the lengthy column name \"height_centimeters\" to \"height_cm\"\n",
    "df = df.withColumnRenamed(\"height_centimeters\",\"height_cm\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"student_transformed.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student_transformed.csv\", header=True, inferSchema=True)\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec51616",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5637a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4198e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec34431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
